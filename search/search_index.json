{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Code Metrics","text":""},{"location":"#about-the-project","title":"About the project","text":"<p>At its core, Code Metrics provides a collection of whole project lifecycle code quality analysis tools. It enables you to combine sources to look for correlations, to answer questions over time such as:</p> <ul> <li>what is my bug to change ratio?</li> <li>which files are frequently implicated when bugs are fixed?</li> <li>how does test coverage correlate to escaped bugs?</li> <li>how has complexity changed with codebase size?</li> <li>how much churn has there been in the codebase (i.e. additions, edits, deletions)</li> </ul> <p>...and many more custom combinations you create.</p>"},{"location":"#learn","title":"Learn","text":"<ul> <li>Getting started</li> <li>Queries</li> <li>Build and deployment pipelines</li> <li>Repository churn</li> <li>Bug culprit files</li> <li>Bugs and escaped bugs</li> <li>Custom queries</li> <li>Workloads</li> <li>Configuration</li> <li>Mocks</li> <li>Datastores</li> <li>Secrets management</li> </ul>"},{"location":"#design-and-build","title":"Design and build","text":"<ul> <li>Architecture</li> <li>Release</li> </ul>"},{"location":"#feature-support","title":"Feature support","text":"<p>See a list of features supported by third party tool.</p>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#high-level-architecture","title":"High level architecture","text":"<p>The tool integrates metrics from your ALM tooling (e.g. Jira), version control system, and code quality tooling (SonarQube).</p> <p></p>"},{"location":"architecture/#technology-overview","title":"Technology overview","text":"<p>The key technologies are Node.js/Express for the API server and Vue.js for the UI. TypeScript is the primary language. Some of the analyses use a backing store (MongoDB is a common choice, but by no means the only possibility).</p> <p>The tool interacts with ALM tooling (Jira), Code quality tools (Sonar) and source control platforms (ADO/Bitbucket/GitHub). These sources provide the raw data for display or subsequent combined analysis.</p> <p>Configuration is done via file and/or environment variables. These provide the details to interact securely with Sonar, Jira, source repository etc. Rules and thresholds are configurable.</p> <p>Packaging is via Docker containers (<code>node:lts</code> for the API server and <code>nginx</code> for static hosting of the UI). Deployment is to anywhere Docker runs, or Node.js if desired.</p>"},{"location":"architecture/#components","title":"Components","text":"<p>See components:</p> <ul> <li>backend</li> <li>ui</li> </ul>"},{"location":"architecture/#repository-groups","title":"Repository groups","text":"<p>Queries use the concept of 'repository groups' (repoGroups).</p> <p>Example repo groups:</p> <ul> <li>backend</li> <li>frontend</li> <li>mobile</li> <li>platform</li> </ul> <p>Repo groups can be defined by either a set of SonarQube tags, or a set of regexes that match repository names.</p> <p>When provided to an API that supports them, repo groups are resolved to a list of repository names. This is done via the sonar tag lookup mechanism, or by matching repository names to specified regexes defined in workload configuration.</p>"},{"location":"authentication/","title":"User authentication configuration","text":"<p>User configuration is represented as password hashes and usernames.</p> <p>These are configured in the <code>users.json</code> file, which should be created in the configuration directory. Copy the example file <code>users.json.example</code> to get started.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>System configuration is defined via a combination of config files and environment variables.</p> <p>These files model how your teams are organised (including rules and thresholds), and contain details to interact securely with the required data sources for Code Quality (e.g. SonarQube), Project Management (e.g. Jira) and Code Management (e.g. ADO).</p>"},{"location":"configuration/#file-format","title":"File format","text":"<p>Configuration files can be in JSON or YAML format. For example, <code>remote-config.yaml</code>, or <code>remote-config.yml</code> or <code>remote-config.json</code>. In this section we refer to the YAML filenames, but the same structure applies to JSON format files in line with its syntax.</p>"},{"location":"configuration/#remote-systems-remote-configyaml","title":"Remote Systems (remote-config.yaml)","text":"<p>Integration configuration with the various third party systems is defined in a file named <code>remote-config.yaml</code>. This file is structured by domain for each of the data sources.</p> <p>Copy the file <code>remote-config.yaml.example</code> and name it <code>remote-config.yaml</code>, then update each application type relevant for your team's tooling setup.</p> <p>The path to the directory containing this file is set by the environment variable <code>CONFIG_DIR</code>. It defaults to the directory containing the <code>backend</code> component, such as <code>/backend</code> in the Docker container. See below.</p> <p>Integrated applications currently supported or planned for future roadmap support include:</p> <p>ALM / Project Management - JIRA (Supported) - Azure DevOps (Roadmap)</p> <p>Code Quality - SonarQube (Supported)</p> <p>Code Management - Azure DevOps (Supported) - Bitbucket (Roadmap) - GitHub (Planned) - GitLab (Roadmap)</p>"},{"location":"configuration/#code-management","title":"Code Management","text":""},{"location":"configuration/#azure-devops-ado","title":"Azure DevOps (ADO)","text":"<ol> <li> <p>Create an Azure Personal Access Token.    To call ADO you'll need to authenticate with a PAT. See instructions here.  </p> </li> <li> <p>Paste the result in your respective <code>remote-config.yaml</code> file configuration within a <code>codeManagement.azure</code> server object's <code>apiKey</code> field.</p> </li> </ol>"},{"location":"configuration/#github","title":"GitHub","text":"<ol> <li>Create a Personal Access Token.    Navigate to https://github.com/settings/tokens and create a token with the following scopes:</li> <li>public_repo</li> <li>read:org</li> <li>read:project</li> <li> <p>repo:status</p> </li> <li> <p>Paste the result in your respective <code>remote-config.json</code> file configuration within a <code>codeManagement.github</code> server object's <code>apiKey</code> field.</p> </li> </ol>"},{"location":"configuration/#code-quality","title":"Code Quality","text":""},{"location":"configuration/#sonar","title":"Sonar","text":"<p>For Sonar, use the SonarQube server URL and token with api permissions generated from the instance.</p> <ol> <li> <p>Using a user with appropriate instance permissions, create an access token within Administration &gt; Users. It is recommended a service user is created for this integration. Further instructions.</p> </li> <li> <p>Paste the result in your respective <code>remote-config.yaml</code> file configuration within a <code>codeAnalysis.sonar</code> server object's <code>apiKey</code> field.  </p> </li> </ol>"},{"location":"configuration/#component-name-prefix","title":"Component name prefix","text":"<p>In some environments, it may be required to prepend a string to component names when querying Sonar. For example, if your repository names are 'frontend' and 'backend' but your Sonar components are named 'projname_frontend' and 'projname_backend', you can set a <code>componentKeyPrefix</code> key in the <code>remote-config.yaml</code> for a given Sonar server.</p>"},{"location":"configuration/#project-management","title":"Project Management","text":""},{"location":"configuration/#jira","title":"Jira","text":"<p>For Jira set the server URL and authentication details, using either:</p> <ul> <li>The email address that you use to access JIRA and API token that you have generated (<code>\"authType\": \"BASIC_AUTH\"</code>), or</li> <li>The API token that you have generated (<code>\"authType\": \"BEARER_TOKEN\"</code>).</li> </ul> <p>To call JIRA you'll need to authenticate with either an API token, or a combination of email address and API token. See instructions here.  </p> <p>Add the JIRA token (and email address if used) to your <code>remote-config.yaml</code> file configuration within a <code>projectManagement.jira</code> server object.</p>"},{"location":"configuration/#workload-teams-projects-workload-configyaml","title":"Workload Teams &amp; Projects (workload-config.yaml)","text":"<p>Workload configuration is held in a file named <code>workload-config.yaml</code>.</p> <p>The path to the directory containing this file is set by the environment variable <code>CONFIG_DIR</code>. It defaults to the directory containing the <code>backend</code> component, such as <code>/backend</code> in the Docker container.</p> <p>To configure your workloads, copy the file <code>workload-config.yaml.example</code> and name it <code>workload-config.yaml</code>.</p> <p>This file contains the structure and hooks used to organise and map the data produced within the remote systems to each team. The information gathered can be aggregated and filtered based on the options provided, see more about features within the other docs. </p>"},{"location":"configuration/#user-authentication","title":"User authentication","text":"<p>See the user authentication section.</p>"},{"location":"configuration/#custom-queries","title":"Custom queries","text":"<p>You can define custom queries using a JSON file. See custom queries for details.</p>"},{"location":"configuration/#application-runtime","title":"Application Runtime","text":"<p>A number of runtime application features are configurable via use of the application environment variables <code>.env</code> file in the working directory. Do this by copying the file <code>.env.template</code> and name the copy <code>.env</code>.</p> <p>Hopefully these are fairly self-explanatory from the variable names provided, with scope of the current settings (non-exhaustive):</p> <ul> <li>Configuration (To define file location &amp; Auto-Reload options) </li> <li>CORS</li> <li>Data Caching, see datastores</li> <li>Logging (Levels, Response Output)</li> <li>System Login (Setting the application <code>admin</code> root user login, also see users)</li> </ul>"},{"location":"custom_queries/","title":"Custom queries","text":"<p>Custom queries are combinations of query types and default inputs that you can use to tailor the tool to a particular analysis, workload or repository. You can define custom queries using a JSON file.</p>"},{"location":"custom_queries/#queries-file","title":"Queries file","text":"<p>Queries are held in a file named <code>queries.json</code>.</p> <p>To customise the queries copy the file <code>queries.json.example</code> and name it <code>queries.json</code>.</p> <p>The path to the directory containing this file is set by the environment variable <code>CONFIG_DIR</code>. It defaults to the directory containing the <code>backend</code> component, such as <code>/backend</code> in the Docker container.</p>"},{"location":"custom_queries/#format","title":"Format","text":"<p>The basic format of the entries in the queries file is:</p> <pre><code>{\n  \"categoryName\": [\n    // array of queries here\n  ]\n}\n</code></pre> <p>A query item looks like this:</p> <ul> <li>name: short user-friendly name</li> <li>description: longer user friendly text</li> <li>component: the type of component (supported: <code>dynamic-input</code>, <code>sonar-metric-summary</code>, <code>file-metric-breakdown</code>)</li> <li>props: the properties for the component</li> </ul> <p>Note that <code>props</code> is dynamic - so, arbitrary properties can be passed through from the config as long as they are supported by the component.</p> <p>For example:</p> <pre><code>{\n  \"name\": \"Team Bugs vs. Coverage\",\n  \"description\": \"Correlates bugs vs. coverage for my team's repositories.\",\n  \"component\": \"dynamic-input\",\n  \"props\": {\n    \"queryTypes\": [\"bugs-new\", \"code-coverage\"],\n    \"defaultInputs\": {\n      \"workloads\": [\"my-team\"]\n    }\n  }\n}\n</code></pre>"},{"location":"datastores/","title":"Datastores","text":"<p>Different datastore implementations are supported.</p> Name Details inmem In-memory datastore implementation.\ufe0f This datastore implementation holds all items in memory for the lifetime of the process. There is no eviction, so growth is infinite with continued insertions. Do not use this in production. mongodb MongoDB datastore. This datastore implementation holds items in an external MongoDB instance. It requires configuration of the connection and authentication details for the MongoDB server."},{"location":"datastores/#configuration","title":"Configuration","text":"<p>Configuration can be set using the <code>DATASTORE_IMPL</code> environment variable in backend, which accepts one of <code>inmem</code>, <code>mongodb</code> as values.</p> <pre><code>DATASTORE_IMPL=inmem\n</code></pre>"},{"location":"datastores/#mongodb-configuration","title":"MongoDB configuration","text":"<p>If the <code>mongodb</code> implementation is used, the following environment variables apply:</p> <pre><code>DATABASE_URI=mongodb://code-metrics:changeme@localhost:27017\nDATABASE_NAME=code-metrics\n</code></pre>"},{"location":"datastores/#caching","title":"Caching","text":"<p>Certain metrics can be cached in the datastore, for rapid subsequent retrieval.</p> <p>Whether the cache is enabled is controlled by this environment variable:</p> <pre><code>LOOKUP_CACHE_ENABLED=true\n</code></pre>"},{"location":"features/","title":"Feature support","text":"<p>This table shows feature support by third party tool.</p> Tool Source Code Management (SCM) CI/CD Pipelines Code Analysis Project Management Azure DevOps \u2705 \u2705 \ud83d\udfe1 Beta Bitbucket #291 GitHub \u2705 \u2705 Gitlab #325 Jenkins \ud83d\udfe1 Beta Jira (Atlassian) \u2705 SonarCloud \u2705 SonarQube \u2705"},{"location":"getting_started/","title":"Getting started","text":"<p>You can run Code Metrics in a number of ways:</p> <ul> <li>Docker or Docker Compose</li> <li>AWS Lambda</li> <li>Using Node.js directly</li> <li>Kubernetes</li> </ul>"},{"location":"getting_started/#docker-compose","title":"Docker Compose","text":"<p>This method uses Docker Compose.</p> <p>To start, clone the repository then run:</p> <pre><code>docker-compose up --build\n</code></pre> <p>(or <code>docker compose up --build</code> if you are using Compose CLI v2).</p> <p>Note: if you want to run with mocked backend services, amend the Compose command as follows: <code>docker-compose -f docker-compose.yaml -f backend/mocks/docker-compose-mocks.yaml up --build</code></p> <p>Access:</p> <ul> <li>Access the web UI at http://localhost:3001</li> <li>The API runs at http://localhost:3000</li> </ul>"},{"location":"getting_started/#kubernetes","title":"Kubernetes","text":"<p>Helm instructions are here: Helm.md</p>"},{"location":"getting_started/#aws-lambda","title":"AWS Lambda","text":"<p>The AWS Lambda deployment can be found on the Releases page.</p> <p>Download the <code>codemetrics-api.zip</code> file. See the example template.yaml for an example of how to deploy the Lambda function.</p> <p>The frontend web UI is a static site, so can be hosted anywhere. You can find the latest version of the web UI on the Releases page.</p> <p>Download the <code>codemetrics-ui.zip</code> file and host it on a static site. You will need to set the <code>apiBaseUrl</code> variable in <code>config.json</code> to point to the API endpoint.</p>"},{"location":"getting_started/#using-nodejs-directly","title":"Using Node.js directly","text":"<p>To run the API and web UI directly, you will need to install Node.js v16 or later.</p> <p>Download the <code>codemetrics-api.zip</code> file and unzip it.</p> <p>To start the API, run:</p> <pre><code>node index.js\n</code></pre> <p>The frontend web UI is a static site, so can be hosted anywhere. You can find the latest version of the web UI on the Releases page.</p> <p>Download the <code>codemetrics-ui.zip</code> file and unzip it. You will need to set the <code>apiBaseUrl</code> variable in <code>config.json</code> to point to the API endpoint.</p>"},{"location":"getting_started/#changing-config-path","title":"Changing config path","text":"<p>To use a different configuration directory, under <code>backend/config</code>, set the <code>CONFIG_DIR</code> environment variable:</p> <pre><code>CONFIG_DIR=/backend/config/somedir docker compose up --build\n</code></pre>"},{"location":"getting_started/#cors-settings","title":"CORS settings","text":"<p>If you need to adjust the origin of the web UI, edit the <code>CORS_ORIGIN</code> environment variable in the <code>backend</code> service in <code>docker-compose.yaml</code>.</p>"},{"location":"helm/","title":"HELM Chart","text":"<p>There are 3 charts to this build: 1. code-metrics-api 2. code-metrics-ui 3. code-metrics</p> <p>The chart code-metrics is an umbrella chart to deploy and confiure the api and ui as a  single deployemt as in 90% of cases they would be deployed together. The inclusion of the  API and UI as subcharts is to enable individual deployments where required.</p>"},{"location":"helm/#limitations-todo","title":"LIMITATIONS / TODO","text":"<ol> <li>All config is currently in <code>code-metrics-api/template/configmap.yaml</code> this needs to be moved to be more dynamic as it currently requires modification of the chart for the purpose of configuration.</li> <li>Passwords etc are stored in ENVVARS and Configmap. This should be moved to secrets.</li> <li>Options for precreated secrets / configmap names unmanaged by helm should be allowed.</li> <li>Naming template is awful at the moment to improve to prevent deployments of -component. <li>Backend readiness healthcheck is the same as the liveness check this should be updated to check dependancies. </li>"},{"location":"helm/#deploying-on-rancher-desktop","title":"Deploying on Rancher desktop","text":"<p>Due to the UI being 'local' and the backend being remote there are some extra steps requried to make it work locally Once deployed the front end with a url using <code>code-metrics.127.0.0.1.sslip.io</code> the backend should be port forwarded to localhost on port 3000. This will allow the UI to talk to the API. In a normal deployment the API would have an ingress that would be used for this purpose but due to DNS and local routing the above workaround is required. Alternatively editing your computers <code>/etc/hosts</code> file with the name and IP may also work but is less 'nice'.</p>"},{"location":"helm/#deploying-in-prod","title":"Deploying in Prod.","text":""},{"location":"prediction/","title":"EXPERIMENTAL FEATURE: Prediction using machine learning","text":"<p>Generates predictions based on one or two input datasets.</p>"},{"location":"prediction/#enabling-the-feature","title":"Enabling the feature","text":"<p>Set the following environment variable in the backend:</p> <pre><code>EXPERIMENTAL_FEATURE_PREDICTIONS=true\n</code></pre>"},{"location":"prediction/#implementation-notes","title":"Implementation notes","text":"<ul> <li>uses TensorFlow</li> <li>supports a maximum of two input datasets</li> </ul>"},{"location":"prediction/#example","title":"Example","text":""},{"location":"prediction/#two-input-datasets","title":"Two input datasets","text":"<p>Here the inputs are lines of code (ncloc) and complexity.</p> <p></p>"},{"location":"prediction/#label-dataset","title":"Label dataset","text":"<p>The label dataset used to train the model is 'number of open bugs'.</p> <p></p>"},{"location":"prediction/#set-up","title":"Set up","text":"<p>Here is the UI with the inputs and labels set up.</p> <p></p>"},{"location":"prediction/#training","title":"Training","text":"<p>When the user hits the <code>Run Prediction</code> button, the backend runs the input queries and the label query as normal. Before a response is returned, however, the input dataset results and the label dataset results are used to train a simple neural network using TensorFlow.</p> <p>The network is trained for 500 iterations, or 'epochs' with a loss function (in this case <code>meanSquaredError</code>) used to optimise the network so its outputs match ever more closely to the label dataset.</p> <p>Once the network has completed its 500 epochs, it is cached in memory so it can be used to make predictions.</p>"},{"location":"prediction/#prediction","title":"Prediction","text":"<p>Once the model is trained, the input datasets are then fed back in, but this time without the label dataset. This time the outputs of the model (its predictions) are recorded and returned to the frontend.</p> <p>In total, the following datasets are returned to the frontend:</p> <ul> <li>input dataset 1 (ncloc)</li> <li>input dataset 2 (complexity)</li> <li>label dataset (open bugs)</li> <li>prediction of open bugs</li> </ul> <p>Here's the predicted value (blue) overlaid on the actual values (red).</p> <p>\u2139\ufe0f In this case, the fit is pretty good, \u26a0\ufe0f but this will not always be the case.</p> <p></p>"},{"location":"queries/","title":"Queries","text":"<p>Information about the different types of queries.</p>"},{"location":"queries/#supported-queries","title":"Supported queries","text":""},{"location":"queries/#source-code-metrics","title":"Source code metrics","text":"<p>Data about the structure, complexity and health of your codebase.</p> <p>Metrics include:</p> <ul> <li>Test coverage</li> <li>Cyclomatic complexity</li> <li>Codebase size (ncloc)</li> </ul>"},{"location":"queries/#build-and-deployment-pipelines","title":"Build and deployment pipelines","text":"<p>Duration, success percentage and outcomes of build and deployment pipelines.</p> <p>Metrics include:</p> <ul> <li>Pipeline outcomes (successful/failed/aborted)</li> <li>Pipeline success percentage</li> <li>Pipeline execution duration</li> </ul>"},{"location":"queries/#repository-churn","title":"Repository churn","text":"<p>A metric showing the amount of change in a repository.</p>"},{"location":"queries/#bug-culprit-files","title":"Bug culprit files","text":"<p>Identifies files that are frequently changed in response to bug fixes. These are potential 'culprits' for code that needs attention.</p>"},{"location":"queries/#bugs-and-escaped-bugs","title":"Bugs and escaped bugs","text":"<p>Bugs/defects from the ALM tool, such as Jira. Helpful to correlate against other software quality metrics.</p>"},{"location":"queries/#query-builder","title":"Query builder","text":"<p>Code Metrics provides a custom query builder, which allows you to combine datasets from one or more query types, such as source code metrics, tickets etc.</p> <p>The query builder supports timeseries datasets.</p> <p>For example, you could chart the following:</p> <ul> <li>what is the bug to change ratio?</li> <li>how does test coverage correlate to escaped bugs?</li> <li>how has complexity changed with codebase size?</li> <li>how much churn has there been in the codebase (i.e. additions, edits, deletions)</li> </ul> <p></p>"},{"location":"queries/#custom-queries","title":"Custom queries","text":"<p>Custom queries are combinations of query types and default inputs that you can use to tailor the tool to a particular analysis, workload or repository.</p> <p>You can define custom queries using a JSON file. See custom queries for details.</p>"},{"location":"queries/#data-points-and-rolling-averages","title":"Data points and rolling averages","text":"<p>You can choose to chart all data points, which is most accurate, but can be quite spiky for some datasets. You can also choose to add rolling averages (over 4 weeks or 12 weeks). </p>"},{"location":"query_bug_culprits/","title":"Bug culprit files","text":"<p>Identifies files that are frequently changed in response to bug fixes. These are potential 'culprits' for code that needs attention.</p> <p>Available dimensions:</p> <ul> <li>workload name</li> </ul>"},{"location":"query_bug_culprits/#ui","title":"UI","text":""},{"location":"query_bugs/","title":"Bugs and escaped bugs","text":"<p>Bugs/defects from the ALM tool, such as Jira. Helpful to correlate against other software quality metrics.</p> <p>Available dimensions:</p> <ul> <li>workload name</li> <li>issue priority</li> <li>escaped (production) only</li> </ul>"},{"location":"query_bugs/#ui","title":"UI","text":""},{"location":"query_bugs/#escaped-bugs","title":"Escaped bugs","text":"<p>Shows all the bugs that 'escaped' to production, by workload, at/above a given priority.</p> <p></p>"},{"location":"query_bugs/#all-bugs","title":"All bugs","text":"<p>Shows all bugs, by workload, at/above a given priority.</p> <p></p>"},{"location":"query_pipelines/","title":"Build and deployment pipelines","text":"<p>Duration, success percentage and outcomes of build and deployment pipelines.</p> <p>Metrics include:</p> <ul> <li>Pipeline outcomes (successful/failed/aborted)</li> <li>Pipeline success percentage</li> <li>Pipeline execution duration</li> </ul> <p>Available dimensions:</p> <ul> <li>Workloads</li> </ul>"},{"location":"query_pipelines/#pipeline-outcomes-successfulfailedaborted","title":"Pipeline outcomes (successful/failed/aborted)","text":""},{"location":"query_pipelines/#pipeline-success-percentage","title":"Pipeline success percentage","text":""},{"location":"query_pipelines/#pipeline-execution-duration","title":"Pipeline execution duration","text":""},{"location":"query_repo_churn/","title":"Repository churn","text":"<p>A metric showing the amount of change in a repository.</p> <p>Available dimensions:</p> <ul> <li>Sonar tags</li> </ul>"},{"location":"query_repo_churn/#calculating-churn","title":"Calculating churn","text":"<p>Churn can be calculated as follows:</p> Strategy Meaning ADD_EDIT_SUBTRACT_DELETE Added lines and edited lines count positively toward the total. Deleted lines subtract from the total. ADD_EDIT_DELETE_CUMULATIVE Added, edited and deleted lines all count positively toward the total. COUNT Ignore number of lines change and count only the instances of a change. i.e. an addition of 10 lines counts as one change. <p>Currently, <code>ADD_EDIT_DELETE_CUMULATIVE</code> is used, to avoid the possibility of a negative value when there are more deletions than additions/edits. Although this would not be problematic in theory, it leads to some unhelpful graph axis issues when overlaying multiple timeseries datasets.</p>"},{"location":"query_repo_churn/#grouping-results","title":"Grouping results","text":"<p>You can group the results by workload, or view results for each repository.</p> <p></p>"},{"location":"query_repo_churn/#with-grouping","title":"With grouping","text":""},{"location":"query_repo_churn/#without-grouping","title":"Without grouping","text":""},{"location":"query_repo_churn/#implementation-notes","title":"Implementation notes","text":""},{"location":"query_repo_churn/#concurrency","title":"Concurrency","text":"<p>Commit retrieval is not very efficient, so a few strategies are used. In future, a wider date range could be provided to the ADO API, however, this would need to be balanced against pulling values from the cache, which might lie in the middle of a given date range.</p> <p>For now, a MongoDB-based store is used to hold a summary of commits and change types (added, edited, deleted) by day, by repo.</p> <p>In addition, to avoid getting rate-limited by the ADO API, a concurrency limit is set (currently 4), using bottleneck.</p>"},{"location":"query_source_code/","title":"Source code metrics","text":"<p>Data about the structure, complexity and health of your codebase.</p> <p>Metrics include:</p> <ul> <li>Test coverage</li> <li>Cyclomatic complexity</li> <li>Lines of code (ncloc)</li> </ul> <p>Available dimensions:</p> <ul> <li>Sonar tags</li> </ul>"},{"location":"query_source_code/#test-coverage-aggregated","title":"Test coverage, aggregated","text":""},{"location":"query_source_code/#test-coverage-timeseries-chart","title":"Test coverage, timeseries chart","text":""},{"location":"query_source_code/#cyclomatic-complexity","title":"Cyclomatic complexity","text":""},{"location":"query_source_code/#lines-of-code-ncloc","title":"Lines of code (ncloc)","text":""},{"location":"release/","title":"Release","text":"<p>Releases are cut from the <code>main</code> branch.</p> <p>To cut a new release perform the following steps.</p>"},{"location":"release/#automated-release","title":"Automated release","text":"<p>The automated release process happens in three parts:</p> <ol> <li>Release script updates version, and creates tag</li> <li>Push to GitHub, where Actions workflow runs tests</li> <li>On successful test execution, Docker images are built and pushed and a GitHub Release is created</li> </ol>"},{"location":"release/#step-1-update-version-and-create-tag","title":"Step 1: Update version and create tag","text":"<p>Run the release prep script:</p> <pre><code>./scripts/prep_release.sh &lt;release type&gt;\n</code></pre> <p>Release type should be one of <code>major</code>, <code>minor</code> or <code>patch</code>, per Semver.</p> <p>For example:</p> <p><code>bash ./scripts/prep_release.sh minor</code></p> <p>This script will:</p> <ul> <li>update the version in <code>package.json</code></li> <li>commit the changes and</li> <li>create a tag for that commit</li> </ul> <p>Note The script does not push to the remote.</p>"},{"location":"release/#step-2-push-to-remote-and-let-github-actions-run","title":"Step 2: Push to remote and let GitHub Actions run","text":"<p>Push the changes to the remote:</p> <pre><code>git push origin main --tags\n</code></pre> <p>View the GitHub Actions workflow at: https://github.com/DeloitteDigitalUK/code-metrics/actions</p> <p>This will:</p> <ul> <li>run tests</li> <li>package assets</li> <li>build and push Docker images</li> <li>create GitHub release</li> </ul> <p>On successful execution, see Releases.</p>"},{"location":"release/#manual-release","title":"Manual release","text":"<p>If you don't want to use the automated release process, use the following steps.</p>"},{"location":"release/#step-1-manually-run-tests","title":"Step 1: Manually run tests","text":"<p>Before you start, check all the tests pass:</p> <p>Backend:</p> <pre><code>$ cd $PROJECT_ROOT/backend\n$ npm test &amp;&amp; npm run test:integration &amp;&amp; npm run test:slow\n</code></pre> <p>Frontend:</p> <pre><code>cd $PROJECT_ROOT/ui\nnpm run test:unit\n</code></pre> <p>Integration:</p> <pre><code>./scripts/validate-test_e2e_mocks.sh\n</code></pre>"},{"location":"release/#step-2-update-the-version-in-backend","title":"Step 2: Update the version in <code>backend</code>","text":"<p>Update the <code>version</code> field in <code>backend/package.json</code>.</p> <p>Notes:</p> <ul> <li>we need to keep the version in backend/package.json up to date with the release</li> <li>this only works when we build a production version of the app</li> <li>UI package version is ignored</li> </ul> <p>Commit this change with a message like:</p> <pre><code>build: release x.y.z.\n</code></pre>"},{"location":"release/#step-3-cut-the-tag","title":"Step 3: Cut the tag","text":"<p>Create a tag in the following format:</p> <pre><code>x.y.z\n</code></pre> <p>For example:</p> <pre><code>1.2.3\n</code></pre> <p>Push your tag to GitHub. The CI/CD pipeline will test the application components, then build and push the Docker images.  </p>"},{"location":"release/#step-4-create-a-github-release","title":"Step 4: Create a GitHub release","text":"<p>Follow the instructions here: https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository#creating-a-release</p>"},{"location":"secret_management/","title":"Managing secrets","text":"<p>Typically, the systems you query with Code Metrics require authentication. You configure how to access these systems using the <code>remote-config.yaml</code> / <code>remote-config.json</code> files in your configuration directory.</p> <p>It is good practice to avoid putting sensitive values, such as API keys, passwords etc. in these files, and instead externalise them to a safe place.</p>"},{"location":"secret_management/#secret-placeholders","title":"Secret placeholders","text":"<p>When you want to refer to a secret in a configuration file, you can use the following syntax:</p> <pre><code>${secret.SECRET_NAME}\n</code></pre> <p>for example:</p> <pre><code>${secret.sonar_api_key}\n</code></pre>"},{"location":"secret_management/#secret-resolution","title":"Secret resolution","text":"<p>When Code Metrics parses your configuration files and it encounters a secret placeholder, it uses a Secret Resolver to obtain the real value.</p> <p>By default, a file-based secrets resolver is used. There are other, more secure, secrets resolvers.</p> <p>Set the resolver to use with the <code>SECRET_RESOLVER_IMPL</code> environment variable.</p> <p>For example:</p> <p><code>SECRET_RESOLVER_IMPL=file</code></p>"},{"location":"secret_management/#file-based-resolver-default","title":"File-based resolver (default)","text":"<p>Set the environment variable:</p> <pre><code>SECRET_RESOLVER_IMPL=file\n</code></pre> <p>This resolver reads a file named <code>secrets.yaml</code> in your config directory.</p> <p>The file is a simple key/value map.</p> <pre><code># secrets.yaml\nsonar_api_key: \"super secret value\"\nanother_secret: \"open sesame\"\n</code></pre> <p>To refer to these secrets in your configuration, use the key of the secret in the file as the secret name, such as: <code>${secret.sonar_api_key}</code></p>"},{"location":"secret_management/#aws-secrets-manager-resolver","title":"AWS Secrets Manager resolver","text":"<p>Set the environment variable:</p> <pre><code>SECRET_RESOLVER_IMPL=secretsmanager\n</code></pre> <p>This resolver queries AWS Secrets Manager using the secret name as the <code>SecretId</code>.</p> <p>To use this resolver, set up your secrets with the correct names and ensure the Code Metrics backend has the necessary AWS permissions (e.g. using IAM or AWS configuration files) to read those secrets' values.</p> <p>The IAM permission required is:</p> <pre><code>secretsmanager:GetSecretValue\n</code></pre> Example IAM policy document  This example IAM policy scopes read access to secrets with names in the format `codemetrics/*`, but you can be a specific as required by your environment.  <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": [\n        \"secretsmanager:GetSecretValue\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"arn:aws:secretsmanager:us-east-1:000000000000:secret:codemetrics/*\"\n    }\n  ]\n}\n</code></pre> <p>To refer to these secrets in your configuration, use the ID of the secret in Secrets Manager as the secret name in the placeholder, such as: <code>${secret.some/secret}</code></p>"},{"location":"workloads/","title":"Workloads","text":"<p>A workload represents one or more repositories, in one or more repository groups. With this concept, you can model a team, an application or a single component.</p>"},{"location":"workloads/#examples","title":"Examples","text":""},{"location":"workloads/#grouping-by-layer","title":"Grouping by layer","text":"<p>Here is how you might model a team with frontend and backend repositories:</p> <pre><code># a team with 'frontend' and 'backend' repositories\n\nworkloads:\n- id: team-athena\n  codeManagement:\n    type: github\n    serverId: example-github\n    projectName: athena\n    repoGroups:\n      backend:\n        repoNames:\n          - example-api\n          - another-api\n      frontend:\n        repoNames:\n          - customer-web\n          - admin-web\n  codeAnalysis:\n    type: sonar\n    serverId: example-sonar\n  projectManagement:\n    type: jira\n    serverId: example-jira\n    project: ATH\n</code></pre> <p>Note Repository group names, such as <code>backend</code> are arbitrary. You can name these groups whatever you like.</p> <p>Here is how you might recreate the previous configuration, by using regular expressions to match repository names:</p> <pre><code># a team with 'frontend' and 'backend' repositories, matched using regular expressions\n\nworkloads:\n- id: team-athena\n  codeManagement:\n    type: github\n    serverId: example-github\n    projectName: athena\n    repoGroups:\n      # match all repositories that end in '-api' using a regular expression\n      backend:\n        repoNames:\n          - /.*-api/\n      # match all repositories that end in '-web' using a regular expression\n      frontend:\n        repoNames:\n          - /.*-web/\n  codeAnalysis:\n    type: sonar\n    serverId: example-sonar\n  projectManagement:\n    type: jira\n    serverId: example-jira\n    project: ATH\n</code></pre> <p>Here is how you might recreate the previous configuration, by using Sonar tags to look up repository names:</p> <pre><code># a team with 'frontend' and 'backend' repositories, looked up via tags in Sonar\n\nworkloads:\n- id: team-athena\n  codeManagement:\n    type: github\n    serverId: example-github\n    projectName: athena\n    repoGroups:\n      # determine the backend repos based on their tags in Sonar\n      backend:\n        sonarTags:\n          - \"be\"\n      # determine the frontend repos based on their tags in Sonar\n      frontend:\n        sonarTags:\n          - \"fe\"\n  codeAnalysis:\n    type: sonar\n    serverId: example-sonar\n  projectManagement:\n    type: jira\n    serverId: example-jira\n    project: ATH\n</code></pre>"},{"location":"workloads/#grouping-by-application","title":"Grouping by application","text":"<p>Here is how you might model a team with multiple applications:</p> <pre><code># a team with multiple applications\n\nworkloads:\n- id: team-hera\n  codeManagement:\n    type: github\n    serverId: example-github\n    projectName: hera\n    repoGroups:\n      account-app:\n        repoNames:\n          - accounts-web\n          - accounts-api\n          - accounts-infra\n      sales-app:\n        repoNames:\n          - sales-portal\n          - sales-api\n          - sales-platform\n  codeAnalysis:\n    type: sonar\n    serverId: example-sonar\n  projectManagement:\n    type: jira\n    serverId: example-jira\n    project: HER\n</code></pre>"},{"location":"workloads/#workload-summary","title":"Workload summary","text":"<p>Each workload has a summary page, showing recent trends.</p> <p>It provides a view of:</p> <ul> <li>all bugs</li> <li>test coverage</li> <li>pipeline success rate</li> </ul> <p></p> <p>You can also view a list of all workloads:</p> <p></p>"}]}